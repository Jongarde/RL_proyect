{
    "activation_fn": "relu",
    "batch_size": 64,
    "clip_range": 0.3,
    "ent_coef": 3.925522106011103e-07,
    "gae_lambda": 0.92,
    "gamma": 0.995,
    "learning_rate": 0.0001571077027830806,
    "max_grad_norm": 0.8,
    "n_epochs": 15,
    "n_steps": 1024,
    "net_arch": "medium",
    "vf_coef": 0.5251286169247003
}